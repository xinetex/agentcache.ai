# AgentCache Plugin - Universal Onboarding Guide

**For**: Developers integrating AgentCache into their AI applications  
**Based on**: Real-world integration experience from SOR Platform (First Production User)  
**Last Updated**: 2024-11-24

This guide helps you integrate AgentCache into your Node.js/Express AI application in **15 minutes or less**.

---

## ğŸ¯ Quick Start (5 Minutes)

### Prerequisites
- Node.js 16+ project
- Existing LLM integration (OpenAI, Anthropic, Google Gemini, etc.)
- Express.js, Next.js, or similar framework

### Step 1: Install AgentCache

```bash
npm install agentcache
# or
yarn add agentcache
# or
pnpm add agentcache
```

### Step 2: Get API Key

1. Sign up at [agentcache.ai](https://agentcache.ai)
2. Create a new API key in your dashboard
3. Copy your key (starts with `ac_live_` or `ac_demo_`)

### Step 3: Add to Environment

```bash
# Add to .env file
echo "AGENTCACHE_API_KEY=ac_live_your_key_here" >> .env
```

### Step 4: Integrate (3 Lines of Code)

**Before** (without caching):
```javascript
const response = await openai.chat.completions.create({
    model: 'gpt-4',
    messages: messages,
});
```

**After** (with caching):
```javascript
import { withCache } from 'agentcache';

const response = await withCache(
    () => openai.chat.completions.create({ model: 'gpt-4', messages }),
    { namespace: `user_${userId}` }
);
```

### Step 5: Verify It Works

Check your console for cache hits:

```
âœ… AgentCache: MISS (1247ms, $0.003)
âœ… AgentCache: HIT (23ms, $0) - Saved $0.003
```

**That's it!** You're now caching LLM responses.

---

## ğŸ“Š See Real Results (Our Experience)

### Before AgentCache
- âŒ Every request: 1.5 seconds latency
- âŒ Every request: $0.001 cost
- âŒ 10 requests/sec max throughput

### After AgentCache (70% hit rate)
- âœ… Cache hits: 50ms latency (30x faster)
- âœ… Cache hits: $0 cost (100% savings)
- âœ… 100+ requests/sec throughput (10x increase)

**Real Monthly Savings**: $20 â†’ $2 (90% reduction)

---

## ğŸ”§ Common Integration Patterns

### Pattern 1: Express.js Chat API

**Scenario**: Basic chat endpoint with authentication

```javascript
import express from 'express';
import { withCache } from 'agentcache';
import OpenAI from 'openai';

const app = express();
const openai = new OpenAI();

app.post('/api/chat', authenticateUser, async (req, res) => {
    const { messages } = req.body;
    const userId = req.user.id;
    
    // Wrap your LLM call with caching
    const completion = await withCache(
        () => openai.chat.completions.create({
            model: 'gpt-4',
            messages: messages,
        }),
        {
            namespace: `user_${userId}`,  // User-scoped caching
            ttl: 604800,                   // 7 days
        }
    );
    
    res.json({ message: completion.choices[0].message });
});
```

**Why This Works**:
- Each user has isolated cache (privacy)
- Same question = instant response
- No code changes to error handling

---

### Pattern 2: RAG (Retrieval-Augmented Generation)

**Scenario**: Chat with context from your documents/database

```javascript
import { withCache } from 'agentcache';

app.post('/api/chat', async (req, res) => {
    const { query } = req.body;
    
    // 1. Retrieve relevant context
    const docs = await searchDocuments(query, { limit: 3 });
    const context = docs.map(d => d.content).join('\n');
    
    // 2. Build prompt with context
    const messages = [
        { role: 'system', content: `Context:\n${context}` },
        { role: 'user', content: query }
    ];
    
    // 3. Cache based on query + context
    const response = await withCache(
        () => openai.chat.completions.create({ model: 'gpt-4', messages }),
        {
            namespace: 'rag',
            cacheKey: `${query}:${context.substring(0, 100)}`, // Include context
        }
    );
    
    res.json({ message: response.choices[0].message });
});
```

**Pro Tip**: Cache invalidation when documents update:
```javascript
// When you update your knowledge base
await agentcache.invalidate({ namespace: 'rag' });
```

---

### Pattern 3: Next.js API Routes

**Scenario**: Next.js 13+ App Router with Server Actions

```javascript
// app/api/chat/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { withCache } from 'agentcache';
import { auth } from '@/lib/auth';

export async function POST(req: NextRequest) {
    const session = await auth();
    const { messages } = await req.json();
    
    const response = await withCache(
        () => openai.chat.completions.create({
            model: 'gpt-4',
            messages: messages,
        }),
        {
            namespace: `user_${session.user.id}`,
        }
    );
    
    return NextResponse.json(response);
}
```

---

### Pattern 4: Streaming Responses

**Scenario**: Real-time streaming chat (like ChatGPT)

```javascript
import { withStreamingCache } from 'agentcache';

app.post('/api/chat/stream', async (req, res) => {
    const { messages } = req.body;
    
    // AgentCache handles streaming automatically
    const stream = await withStreamingCache(
        () => openai.chat.completions.create({
            model: 'gpt-4',
            messages: messages,
            stream: true,
        }),
        {
            namespace: `user_${req.user.id}`,
            onCacheHit: (chunks) => {
                // Replay cached stream
                chunks.forEach(chunk => res.write(chunk));
                res.end();
            }
        }
    );
    
    // On cache miss, stream normally
    for await (const chunk of stream) {
        res.write(JSON.stringify(chunk));
    }
    res.end();
});
```

---

### Pattern 5: Multi-Model with Fallback

**Scenario**: Try cheap model first, fallback to expensive one

```javascript
import { withCache } from 'agentcache';

async function chatWithFallback(messages, userId) {
    try {
        // Try GPT-3.5 first (cheap)
        return await withCache(
            () => openai.chat.completions.create({
                model: 'gpt-3.5-turbo',
                messages: messages,
            }),
            { namespace: `user_${userId}:gpt35` }
        );
    } catch (error) {
        console.log('GPT-3.5 failed, using GPT-4');
        
        // Fallback to GPT-4 (expensive)
        return await withCache(
            () => openai.chat.completions.create({
                model: 'gpt-4',
                messages: messages,
            }),
            { namespace: `user_${userId}:gpt4` }
        );
    }
}
```

---

## ğŸ” Security Best Practices

### 1. User Isolation (Critical for Multi-Tenant Apps)

**Always use namespaces** to prevent users from seeing each other's cached responses:

```javascript
// âœ… Good - User-scoped
await withCache(llmCall, { 
    namespace: `user_${userId}` 
});

// âŒ Bad - Global cache (privacy leak!)
await withCache(llmCall, { 
    namespace: 'global' 
});
```

### 2. Sensitive Data Handling

**Enable encryption** for cached responses containing PII:

```javascript
import { AgentCache } from 'agentcache';

const cache = new AgentCache({
    apiKey: process.env.AGENTCACHE_API_KEY,
    encryptContent: true,                    // Encrypt at rest
    encryptionKey: process.env.CACHE_ENCRYPTION_KEY,
});
```

### 3. Compliance (GDPR, FERPA, COPPA)

**Right to be forgotten**:

```javascript
// When user deletes account
await agentcache.invalidate({ 
    namespace: `user_${userId}`,
    reason: 'user_deletion' 
});
```

**Audit logging**:

```javascript
await withCache(llmCall, {
    namespace: `user_${userId}`,
    auditLog: {
        userId: userId,
        action: 'chat_request',
        ipAddress: req.ip,
        timestamp: new Date(),
    }
});
```

---

## ğŸ“ˆ Monitoring & Observability

### Track Cache Performance

```javascript
import { AgentCache } from 'agentcache';

const cache = new AgentCache({
    onCacheHit: (key, latency, costSaved) => {
        console.log(`ğŸ’š Cache hit: ${key} in ${latency}ms (saved $${costSaved})`);
        
        // Send to your monitoring (DataDog, New Relic, etc.)
        metrics.increment('agentcache.hits');
        metrics.histogram('agentcache.latency', latency);
        metrics.increment('agentcache.cost_saved', costSaved);
    },
    
    onCacheMiss: (key, latency, cost) => {
        console.log(`ğŸ”´ Cache miss: ${key} in ${latency}ms (cost $${cost})`);
        
        metrics.increment('agentcache.misses');
        metrics.histogram('agentcache.llm_latency', latency);
        metrics.increment('agentcache.cost', cost);
    },
});
```

### Dashboard Integration

```javascript
// Get stats for your admin dashboard
const stats = await agentcache.getStats({
    period: '7d',
    namespace: `user_${userId}`,
});

console.log(stats);
// {
//   totalRequests: 1000,
//   cacheHits: 700,
//   hitRate: 0.70,
//   tokensSaved: 1400000,
//   costSaved: '$14.50',
//   avgLatency: 45,
// }
```

---

## âš¡ Advanced Configuration

### Semantic Caching (Beta)

**Cache "similar" queries**, not just exact matches:

```javascript
import { withSemanticCache } from 'agentcache';

// "What is Python?" and "Explain Python" = same cache entry
const response = await withSemanticCache(
    () => openai.chat.completions.create({ model: 'gpt-4', messages }),
    {
        namespace: 'docs',
        similarityThreshold: 0.90,  // 90% similar = cache hit
    }
);
```

**Use Case**: Documentation bots, FAQ systems, education platforms

---

### Custom TTL per Query Type

```javascript
// FAQ answers: cache 30 days (stable)
await withCache(faqQuery, {
    namespace: 'faq',
    ttl: 2592000,
});

// News summaries: cache 1 hour (volatile)
await withCache(newsQuery, {
    namespace: 'news',
    ttl: 3600,
});

// User chat: cache 7 days (default)
await withCache(chatQuery, {
    namespace: `user_${userId}`,
    ttl: 604800,
});
```

---

### Manual Cache Control

```javascript
import { AgentCache } from 'agentcache';

const cache = new AgentCache();

// Check if cached (without retrieving)
const isCached = await cache.check({
    provider: 'openai',
    model: 'gpt-4',
    messages: messages,
});

// Get from cache manually
const cached = await cache.get({
    provider: 'openai',
    model: 'gpt-4',
    messages: messages,
    namespace: `user_${userId}`,
});

if (cached) {
    console.log('Cache hit!', cached.response);
} else {
    const response = await openai.chat.completions.create(...);
    
    // Store in cache manually
    await cache.set({
        provider: 'openai',
        model: 'gpt-4',
        messages: messages,
        response: response,
        namespace: `user_${userId}`,
        ttl: 604800,
    });
}
```

---

## ğŸ› Troubleshooting

### Issue 1: Cache Always Misses

**Symptoms**: Every request is a cache miss, no hits.

**Common Causes**:
1. Namespace changes between requests
2. Message format inconsistency (extra fields)
3. Temperature/params changing

**Fix**:
```javascript
// âŒ Bad - namespace changes every request
await withCache(llmCall, { 
    namespace: `user_${Date.now()}` 
});

// âœ… Good - stable namespace
await withCache(llmCall, { 
    namespace: `user_${userId}` 
});

// âŒ Bad - inconsistent message format
messages.push({ timestamp: Date.now() }); // Changes every time!

// âœ… Good - consistent format
const cacheKey = messages.map(m => m.content).join('|');
```

---

### Issue 2: High Latency on Hits

**Symptoms**: Cache hits take 200ms+ (should be <50ms).

**Common Causes**:
1. Wrong Upstash region
2. Network issues
3. Large cached responses

**Fix**:
```javascript
// Check your Upstash region matches your deployment
const cache = new AgentCache({
    region: 'us-east-1',  // Match your Vercel/AWS region
});

// Or compress large responses
const cache = new AgentCache({
    compression: true,  // Reduces cache size by 70%
});
```

---

### Issue 3: "API Key Invalid" Error

**Symptoms**: `Error: AgentCache API key invalid`

**Fix**:
```bash
# Check your .env file
cat .env | grep AGENTCACHE

# Should see:
AGENTCACHE_API_KEY=ac_live_...  # NOT ac_demo_test123

# Verify key is loaded
node -e "console.log(process.env.AGENTCACHE_API_KEY)"
```

**Note**: Demo keys (`ac_demo_test123`) have 20 requests/hour limit.

---

### Issue 4: Streaming Not Working

**Symptoms**: Streaming responses don't cache or replay incorrectly.

**Fix**:
```javascript
// Use streaming-specific wrapper
import { withStreamingCache } from 'agentcache';

// Not the regular withCache()
const stream = await withStreamingCache(
    () => openai.chat.completions.create({ stream: true, ... }),
    { namespace: `user_${userId}` }
);
```

---

### Issue 5: Cache Not Invalidating

**Symptoms**: Updated data still returns old cached responses.

**Fix**:
```javascript
// Invalidate specific namespace
await agentcache.invalidate({ 
    namespace: 'docs' 
});

// Invalidate by pattern
await agentcache.invalidate({ 
    pattern: 'user_*' 
});

// Invalidate by age
await agentcache.invalidate({ 
    olderThan: 86400000  // 24 hours in ms
});
```

---

## ğŸ“š Real-World Example (Complete)

Here's our **actual production code** from the SOR Platform:

```javascript
// server/routes/chat.js (simplified)
import express from 'express';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { withCache } from 'agentcache';
import { authenticateToken } from '../middleware/auth.js';
import { searchSimilarContent } from '../services/embeddingService.js';

const router = express.Router();
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

router.post('/', authenticateToken, async (req, res) => {
    const { messages } = req.body;
    const userId = req.user.id;
    const userRole = req.user.role;
    
    // RAG: Get relevant research content
    const lastMessage = messages[messages.length - 1].content;
    const researchContext = await searchSimilarContent(lastMessage, 3);
    
    // Build system prompt with context
    const systemPrompt = buildSystemPrompt(userRole, researchContext);
    
    // Cached LLM call
    const response = await withCache(
        async () => {
            const model = genAI.getGenerativeModel({ 
                model: 'gemini-1.5-flash',
                systemInstruction: systemPrompt,
            });
            
            const chat = model.startChat({ history: messages.slice(0, -1) });
            const result = await chat.sendMessage(lastMessage);
            return result.response.text();
        },
        {
            namespace: `user_${userId}`,
            ttl: 604800,  // 7 days
            cacheKey: `${lastMessage}:${systemPrompt.substring(0, 100)}`,
        }
    );
    
    res.json({ 
        message: { role: 'assistant', content: response },
        citations: researchContext.map(r => r.source),
    });
});

export default router;
```

**Results**:
- Cache hit rate: 72%
- P95 latency: 38ms (was 1500ms)
- Cost: $2/month (was $20/month)

---

## ğŸ“ Best Practices Checklist

Before deploying to production:

- [ ] **Namespaces**: User-scoped for multi-tenant apps
- [ ] **Error Handling**: Graceful degradation if cache fails
- [ ] **Monitoring**: Track hit rate, latency, cost savings
- [ ] **Security**: Encryption enabled for sensitive data
- [ ] **TTL Strategy**: Appropriate expiration per content type
- [ ] **Invalidation**: Clear cache when source data updates
- [ ] **Testing**: Verify cache hits in staging environment
- [ ] **Documentation**: Team knows how caching works
- [ ] **Compliance**: GDPR/FERPA requirements met
- [ ] **Billing**: Monitor cache API usage vs limits

---

## ğŸ’° Cost Optimization Tips

### Tip 1: Global Cache for Public Content

```javascript
// Public FAQ: share cache across all users
await withCache(faqQuery, {
    namespace: 'public_faq',  // Not user-scoped
});

// Hit rate: 95% â†’ massive savings
```

### Tip 2: Preload Common Queries

```javascript
// Cache popular questions during off-peak hours
const popularQuestions = [
    'What is phonemic awareness?',
    'How do I teach blending?',
    'What are decodable books?',
];

for (const question of popularQuestions) {
    await withCache(
        () => llm.chat([{ role: 'user', content: question }]),
        { namespace: 'preloaded' }
    );
}

// Users get instant responses for common questions
```

### Tip 3: Longer TTL for Stable Content

```javascript
// Research content: 30 days (rarely changes)
await withCache(researchQuery, { ttl: 2592000 });

// Chat responses: 7 days (default)
await withCache(chatQuery, { ttl: 604800 });

// News/events: 1 hour (volatile)
await withCache(newsQuery, { ttl: 3600 });
```

---

## ğŸš€ Going to Production

### Production Checklist

1. **Switch from Demo to Live Key**
   ```bash
   # Replace in .env
   AGENTCACHE_API_KEY=ac_live_prod_xxx
   ```

2. **Set Up Monitoring**
   ```javascript
   // Add to error tracking (Sentry, Rollbar, etc.)
   cache.onError((error) => {
       Sentry.captureException(error, {
           tags: { service: 'agentcache' }
       });
   });
   ```

3. **Configure Upstash Region**
   ```javascript
   // Match your deployment region for lowest latency
   const cache = new AgentCache({
       region: process.env.UPSTASH_REGION || 'us-east-1',
   });
   ```

4. **Set Up Alerts**
   ```javascript
   // Alert if hit rate drops below 50%
   if (stats.hitRate < 0.50) {
       alertTeam('AgentCache hit rate low: ' + stats.hitRate);
   }
   ```

5. **Document for Team**
   - Add README section on caching
   - Document namespace conventions
   - Note invalidation triggers

---

## ğŸ“ Getting Help

### Community Support
- **Discord**: [discord.gg/agentcache](https://discord.gg/agentcache)
- **GitHub Issues**: [github.com/jettythunder/agentcache-ai](https://github.com/jettythunder/agentcache-ai)
- **Docs**: [agentcache.ai/docs](https://agentcache.ai/docs)

### Email Support
- **General**: support@agentcache.ai
- **Enterprise**: sales@agentcache.ai
- **Security**: security@agentcache.ai

### Example Projects
- **SOR Platform**: [github.com/jettythunder/sor-app](https://github.com/jettythunder/sor-app) (First production user)
- **More examples**: Coming soon as more teams integrate

---

## ğŸ‰ Success Stories

### Case Study: SOR Platform

**Before AgentCache**:
- 1,000 requests/day
- 1.5s average latency
- $20/month Gemini costs
- 10 req/sec max throughput

**After AgentCache**:
- 1,000 requests/day
- 150ms average latency (10x faster)
- $2/month total costs (90% reduction)
- 100 req/sec throughput (10x increase)

**Integration Time**: 2 hours  
**Payback Period**: Immediate (free tier sufficient)

> "AgentCache reduced our AI costs by 90% and made our chat 10x faster. Integration took 2 hours." - JettyThunder Labs

---

## ğŸ”„ Migration from Other Solutions

### From Redis

```javascript
// Before (manual Redis)
const cached = await redis.get(cacheKey);
if (cached) return JSON.parse(cached);

const response = await llm.chat(messages);
await redis.setex(cacheKey, 3600, JSON.stringify(response));
return response;

// After (AgentCache)
return await withCache(
    () => llm.chat(messages),
    { namespace: 'chat', ttl: 3600 }
);
```

**Benefits**: No Redis management, semantic caching, global edge network

### From Local Cache (LRU, etc.)

```javascript
// Before (node-cache)
const NodeCache = require('node-cache');
const cache = new NodeCache({ stdTTL: 3600 });

const cached = cache.get(key);
if (cached) return cached;

const response = await llm.chat(messages);
cache.set(key, response);
return response;

// After (AgentCache)
return await withCache(
    () => llm.chat(messages),
    { namespace: 'chat' }
);
```

**Benefits**: Shared across instances, persistent, analytics

---

## ğŸ“– Additional Resources

- **[Integration Requirements Doc](./AGENTCACHE_INTEGRATION_REQUIREMENTS.md)** - Technical requirements for plugin development
- **[AgentCache Website](https://agentcache.ai)** - Product overview and pricing
- **[MCP Documentation](https://modelcontextprotocol.io)** - Model Context Protocol spec
- **[Upstash Docs](https://upstash.com/docs)** - Redis and Vector DB docs

---

## ğŸ“ Feedback & Improvements

This guide is based on real integration experience from the first production user (SOR Platform). If you have:
- âœ… Success stories to share
- ğŸ› Issues not covered here
- ğŸ’¡ Suggestions for improvement
- ğŸ“š Additional patterns to document

Please contribute at: [github.com/jettythunder/agentcache-ai/docs](https://github.com/jettythunder/agentcache-ai)

---

**Happy Caching! ğŸš€ğŸ’°**

*Last updated by: JettyThunder Labs (First Production User)*  
*Version: 1.0.0*  
*Date: 2024-11-24*
