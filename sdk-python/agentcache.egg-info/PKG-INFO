Metadata-Version: 2.4
Name: agentcache
Version: 0.3.0
Summary: Official Python client for AgentCache.ai - Reduce LLM costs by 90%
Home-page: https://github.com/xinetex/agentcache.ai
Author: AgentCache.ai
Author-email: support@agentcache.ai
Project-URL: Bug Tracker, https://github.com/xinetex/agentcache.ai/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.25.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: project-url
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# AgentCache Python Client

Official Python client for [AgentCache.ai](https://agentcache.ai) - The Global Edge Cache for LLMs.

## Installation

```bash
pip install agentcache
```

## Usage

### Standard Completion

```python
import agentcache

# Drop-in replacement for OpenAI call logic
response = agentcache.completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello world"}],
    provider="openai"  # optional, defaults to openai
)

if response and response.get('hit'):
    print("Cache HIT:", response['response'])
else:
    print("Cache MISS - Call your LLM here")
```

### Streaming

AgentCache supports streaming responses, making it compatible with chat UIs that expect Server-Sent Events (SSE).

```python
stream = agentcache.completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Write a poem"}],
    stream=True
)

if stream:
    print("Cache HIT (Streaming):")
    for chunk in stream:
        content = chunk['choices'][0]['delta'].get('content', '')
        print(content, end="", flush=True)
```

### Reasoning Cache (NEW in v0.3.0)

Cache reasoning traces for o1, Kimi, and DeepSeek models:

```python
response = agentcache.completion(
    model="o1-preview",
    messages=[{"role": "user", "content": "Analyze this contract..."}],
    strategy="reasoning_cache"
)

if response and response.get('cached'):
    print("Reasoning trace retrieved from cache")
```

### Multimodal Cache (NEW in v0.3.0)

Cache 3D meshes, images, and audio for generative models:

```python
response = agentcache.completion(
    model="sam-3d-body",
    messages=[{
        "role": "user",
        "content": "Generate 3D model",
        "file_path": "input.jpg"
    }],
    strategy="multimodal"
)

if response and response.get('cached'):
    asset_data = response['asset']
    print(f"Retrieved cached 3D asset: {len(asset_data['vertices'])} vertices")
```

## Environment Variables

Set `AGENTCACHE_API_KEY` in your environment, or pass `api_key` to the constructor.

```python
from agentcache import AgentCache

client = AgentCache(api_key="ac_live_...")
```
